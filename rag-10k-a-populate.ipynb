{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  RAG-1A - Doing RAG with MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CUDA/GPU:  True\n",
      "device  0 NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "## Check if GPU is enabled\n",
    "import os\n",
    "import torch\n",
    "\n",
    "## To disable GPU and experiment, uncomment the following line\n",
    "## Normally, you would want to use GPU, if one is available.\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "print (\"using CUDA/GPU: \", torch.cuda.is_available())\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(\"device \", i , torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup logging.  To see more loging set the level to DEBUG\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1: Load Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "## Load Settings from .env file\n",
    "from dotenv import find_dotenv, dotenv_values\n",
    "\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "config = dotenv_values(find_dotenv())\n",
    "\n",
    "# debug\n",
    "# print (config)\n",
    "\n",
    "ATLAS_URI = config.get('ATLAS_URI')\n",
    "\n",
    "if not ATLAS_URI:\n",
    "    raise Exception (\"'ATLAS_URI' is not set.  Please set it above to continue...\")\n",
    "\n",
    "## Only need this if we are using OpenAI for Embeddings\n",
    "# OPENAI_API_KEY = config.get(\"OPENAI_API_KEY\")\n",
    "# if not OPENAI_API_KEY:\n",
    "#     raise Exception (\"'OPENAI_API_KEY' is not set.  Please set it above to continue...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = 'rag1'\n",
    "COLLECTION_NAME = '10k'\n",
    "INDEX_NAME = 'idx_embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "## LlamaIndex will download embeddings models as needed.\n",
    "## Set llamaindex cache dir to ./cache dir here (Default is system tmp)\n",
    "## This way, we can easily see downloaded artifacts\n",
    "os.environ['LLAMA_INDEX_CACHE_DIR'] = os.path.join(os.path.abspath(''), 'cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas client initialized\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "\n",
    "mongodb_client = pymongo.MongoClient(ATLAS_URI)\n",
    "\n",
    "print (\"Atlas client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear out the collection\n",
    "\n",
    "For a fresh start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document count before delete : 754\n",
      "Deleted docs : 754\n"
     ]
    }
   ],
   "source": [
    "database = mongodb_client[DB_NAME]\n",
    "collection = database [COLLECTION_NAME]\n",
    "\n",
    "doc_count = collection.count_documents (filter = {})\n",
    "print (f\"Document count before delete : {doc_count:,}\")\n",
    "\n",
    "result = collection.delete_many(filter= {})\n",
    "print (f\"Deleted docs : {result.deleted_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Embeddings\n",
    "\n",
    "The default embedding is OpenAI.  We can always plugin custom embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Embeddings\n",
    "\n",
    "This is using OpenAI embedding model\n",
    "You will need an API key (defined in env variable : OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index import  OpenAIEmbedding\n",
    "# embed_model = OpenAIEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Custom Embeddings\n",
    "\n",
    "Here are a select models for comparison.  Taken from leaderboard : https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "| model name                              | overall score | model size | model params | embedding length | License  | url                                                            |\n",
    "|-----------------------------------------|---------------|------------|--------------|------------------|----------|----------------------------------------------------------------|\n",
    "| BAAI/bge-large-en-v1.5                  | 64.x          | 1.34 GB    | 335 M        | 1024             | MIT      | https://huggingface.co/BAAI/bge-large-en-v1.5                  |\n",
    "| BAAI/bge-small-en-v1.5                  | 62.x          | 133 MB     | 33.5 M       | 384              | MIT      | https://huggingface.co/BAAI/bge-small-en-v1.5                  |\n",
    "| sentence-transformers/all-mpnet-base-v2 | 57.8          | 438 MB     |              | 768              | Apache 2 | https://huggingface.co/sentence-transformers/all-mpnet-base-v2 |\n",
    "| sentence-transformers/all-MiniLM-L12-v2 | 56.x          | 134 MB     |              | 384              | Apache 2 | https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2 |\n",
    "| sentence-transformers/all-MiniLM-L6-v2  | 56.x          | 91 MB      |              | 384              | Apache 2 | https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sujee/anaconda3/envs/atlas-2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "## setup embed model\n",
    "\n",
    "# The LLM used to generate natural language responses to queries.\n",
    "# If not provided, defaults to gpt-3.5-turbo from OpenAI\n",
    "# If your OpenAI key is not set, defaults to llama2-chat-13B from Llama.cpp\n",
    "# We don't need an LLM just yet, so setting it to None\n",
    "\n",
    "from llama_index import  ServiceContext\n",
    "\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect Illama-Index and MongoDB Atlas\n",
    "\n",
    "Let's define MongoDB Atlas as our vector storage. This is critical to stored indexed data and then query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(mongodb_client = mongodb_client,\n",
    "                                 db_name = DB_NAME, collection_name = COLLECTION_NAME,\n",
    "                                 index_name  = 'idx_embedding',\n",
    "                                 ## the following columns are set to default values\n",
    "                                 # embedding_key = 'embedding', text_key = 'text', metadata_= 'metadata',\n",
    "                                 )\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Documents\n",
    "\n",
    "Ilmaa-index has very handy `SimpleDirectoryReader` that can read single files / multiple files / or entire directory content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 545 chunks from './data/10k/'\n",
      "CPU times: user 10.1 s, sys: 13 ms, total: 10.1 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from llama_index.readers.file.base import SimpleDirectoryReader\n",
    "\n",
    "data_dir = './data/10k/'\n",
    "\n",
    "## This reads one doc\n",
    "# docs = SimpleDirectoryReader(\n",
    "#     input_files=[\"./data/10k/uber_2021.pdf\"]\n",
    "# ).load_data()\n",
    "\n",
    "## here we read entire directory content\n",
    "docs = SimpleDirectoryReader(\n",
    "        input_dir=data_dir\n",
    ").load_data()\n",
    "\n",
    "print (f\"Loaded {len(docs)} chunks from '{data_dir}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index the docs and Store \n",
    "\n",
    "When we execute the code below, the following will happen\n",
    "\n",
    "- documents are indexed\n",
    "- embeddings are created for text\n",
    "- the document (text, embeddings) are stored in our Vector Storage (MongoDB Atlas in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.86 s, sys: 341 ms, total: 10.2 s\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from llama_index.indices.vector_store.base import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    docs, storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Index\n",
    "\n",
    "Before we do search, we need to define an embedding index\n",
    "\n",
    "You can look at steps here [setup-atlas-index.md](setup-atlas-index.md)\n",
    "\n",
    "Here are the details:\n",
    "\n",
    "index_name = 'idx_embedding'\n",
    "\n",
    "index defitintion json\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"type\": \"vector\",\n",
    "      \"path\": \"embedding\",\n",
    "      \"numDimensions\": 384,\n",
    "      \"similarity\": \"dotProduct\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "The similarity function can be  one of \n",
    "- \"euclidean\"\n",
    "- \"cosine\"\n",
    "- \"dotProduct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are done \n",
    "\n",
    "Now the data is populated and ready to be queried.\n",
    "\n",
    "Let's go to the next lab: query"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
