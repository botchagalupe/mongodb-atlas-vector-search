{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  RAG 10k Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if GPU is enabled\n",
    "import os\n",
    "import torch\n",
    "\n",
    "## To disable GPU and experiment, uncomment the following line\n",
    "## Normally, you would want to use GPU, if one is available.\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "print (\"using CUDA/GPU: \", torch.cuda.is_available())\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(\"device \", i , torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup logging.  To see more loging set the level to DEBUG\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1: Load Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "## Load Settings from .env file\n",
    "from dotenv import find_dotenv, dotenv_values\n",
    "\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "config = dotenv_values(find_dotenv())\n",
    "\n",
    "# debug\n",
    "# print (config)\n",
    "\n",
    "ATLAS_URI = config.get('ATLAS_URI')\n",
    "\n",
    "if not ATLAS_URI:\n",
    "    raise Exception (\"'ATLAS_URI' is not set.  Please set it above to continue...\")\n",
    "\n",
    "## Only need this if we are using OpenAI for Embeddings\n",
    "OPENAI_API_KEY = config.get(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise Exception (\"'OPENAI_API_KEY' is not set.  Please set it above to continue...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = 'rag1'\n",
    "COLLECTION_NAME = '10k'\n",
    "INDEX_NAME = 'idx_embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "## LlamaIndex will download embeddings models as needed.\n",
    "## Set llamaindex cache dir to ./cache dir here (Default is system tmp)\n",
    "## This way, we can easily see downloaded artifacts\n",
    "os.environ['LLAMA_INDEX_CACHE_DIR'] = os.path.join(os.path.abspath(''), 'cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas client initialized\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "mongodb_client = MongoClient(ATLAS_URI)\n",
    "\n",
    "print (\"Atlas client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Embeddings\n",
    "\n",
    "The default embedding is OpenAI.  We can always plugin custom embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Embeddings\n",
    "\n",
    "This is using OpenAI embedding model\n",
    "You will need an API key (defined in env variable : OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index import  OpenAIEmbedding\n",
    "# embed_model = OpenAIEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Custom Embeddings\n",
    "\n",
    "Remember this embedding model must be the same as in `populate` step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sujee/anaconda3/envs/atlas-2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import  ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "# The LLM used to generate natural language responses to queries.\n",
    "# If not provided, defaults to gpt-3.5-turbo from OpenAI\n",
    "# If your OpenAI key is not set, defaults to llama2-chat-13B from Llama.cpp\n",
    "\n",
    "## Here are the models available : https://platform.openai.com/docs/models\n",
    "##  gpt-3.5-turbo  |   gpt-4  | gpt-4-turbo\n",
    "\n",
    "#  | model         | context window |\n",
    "#  |---------------|----------------|\n",
    "#  | gpt-3.5-turbo | 4,096          |\n",
    "#  | gpt-4         | 8,192          |\n",
    "#  | gpt-4-turbo   | 128,000        |\n",
    "\n",
    "\n",
    "## set temperature=0 for predictable results\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "## setup embed model\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect Illama-Index and MongoDB Atlas\n",
    "\n",
    "Let's define MongoDB Atlas as our vector storage. This is critical to stored indexed data and then query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.indices.vector_store.base import VectorStoreIndex\n",
    "\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(mongodb_client = mongodb_client,\n",
    "                                 db_name = DB_NAME, collection_name = COLLECTION_NAME,\n",
    "                                 index_name  = 'idx_embedding',\n",
    "                                 ## the following columns are set to default values\n",
    "                                 # embedding_key = 'embedding', text_key = 'text', metadata_= 'metadata',\n",
    "                                 )\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Data / Ask Questions\n",
    "\n",
    "Now that we have every thing setup, let's ask some questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uber's revenue for the years ended December 31, 2019, 2020, and 2021 was $13,000 million, $11,139 million, and $17,455 million, respectively.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "response = index.as_query_engine().query(\"What was Uber's revenue?\")\n",
    "print (response)\n",
    "# display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyft made $2,364,681 in revenue in 2020.\n"
     ]
    }
   ],
   "source": [
    "response = index.as_query_engine().query(\"How much money Lyft made in 2020?\")\n",
    "print (response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I cannot provide the answer to your query.\n"
     ]
    }
   ],
   "source": [
    "## The answer to this question doesn't exist in the Lyft_10k filing!\n",
    "## Let's see what we get back\n",
    "response = index.as_query_engine().query(\"How much money Lyft made in 2018?\")\n",
    "print (response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uber did its initial public offering (IPO) on May 14, 2019.\n"
     ]
    }
   ],
   "source": [
    "response = index.as_query_engine().query(\"When did Uber do IPO?\")\n",
    "print (response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
