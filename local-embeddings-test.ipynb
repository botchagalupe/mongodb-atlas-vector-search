{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Embeddings\n",
    "\n",
    "We will try a few embedding models running locally and compare their performance\n",
    "\n",
    "## References\n",
    "\n",
    "- https://docs.llamaindex.ai/en/stable/examples/embeddings/huggingface.html#huggingfaceembedding\n",
    "- Leaderboard : https://huggingface.co/spaces/mteb/leaderboard\n",
    "- Explaining leaderboard: https://huggingface.co/blog/mteb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU\n",
    "\n",
    "GPUs can really accellerate LLM / embedding operations.  Let's make sure we are using GPUs if we have them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CUDA/GPU:  True\n",
      "device  0 NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "## To disable GPU and experiment, uncomment the following line\n",
    "## Normally, you would want to use GPU, if one is available.\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "print (\"using CUDA/GPU: \", torch.cuda.is_available())\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(\"device \", i , torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging\n",
    "\n",
    "To see every thing set the logging level to DEBUG.  \n",
    "To disable all logs set the logging level to ERROR or WARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup logging\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1 - Using Llama-Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sujee/anaconda3/envs/atlas-2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "[-0.0032757227309048176, -0.011690807528793812, 0.041559189558029175, -0.03814816102385521, 0.024183066561818123]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "## Optional, but set llamaindex cache dir to ./cache dir here.  Default is system tmp\n",
    "## This way, we can easily see downloaded artifacts\n",
    "os.environ['LLAMA_INDEX_CACHE_DIR'] = os.path.join(os.path.abspath(''), 'cache')\n",
    "\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
    "\n",
    "print(len(embeddings))\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try a few embedding models\n",
    "\n",
    "See hugging face embedding models (sentence transformers) here : https://huggingface.co/models?library=sentence-transformers&sort=trending\n",
    "\n",
    "Here are a select models for comparison.  Taken from leaderboard : https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "| model name                              | overall score | model size | model params | embedding length | License  | url                                                            |\n",
    "|-----------------------------------------|---------------|------------|--------------|------------------|----------|----------------------------------------------------------------|\n",
    "| intfloat/e5-mistral-7b-instruct         | 66.x          | 15 GB      | 7.11 B       | 4096             | MIT      | https://huggingface.co/intfloat/e5-mistral-7b-instruct         |\n",
    "| BAAI/bge-large-en-v1.5                  | 64.x          | 1.34 GB    | 335 M        | 1024             | MIT      | https://huggingface.co/BAAI/bge-large-en-v1.5                  |\n",
    "| BAAI/bge-small-en-v1.5                  | 62.x          | 133 MB     | 33.5 M       | 384              | MIT      | https://huggingface.co/BAAI/bge-small-en-v1.5                  |\n",
    "| sentence-transformers/all-mpnet-base-v2 | 57.8          | 438 MB     |              | 768              | Apache 2 | https://huggingface.co/sentence-transformers/all-mpnet-base-v2 |\n",
    "| sentence-transformers/all-MiniLM-L12-v2 | 56.x          | 134 MB     |              | 384              | Apache 2 | https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2 |\n",
    "| sentence-transformers/all-MiniLM-L6-v2  | 56.x          | 91 MB      |              | 384              | Apache 2 | https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "\n",
    "Benchmarks are fun way to evalute the performance of embedding models.\n",
    "\n",
    "Here we use Python's `%timeit` command.  it runs the code a few times and calcualtes the average.\n",
    "\n",
    "The following code block will calculate benchmark numbers.\n",
    "\n",
    "Here are the benchmark numbers for encoding \"Hello World\" on my desktop \n",
    "- Ubuntu 22.04 with Nvidia CUDA drivers (Driver Version: 525.147.05   CUDA Version: 12.0)\n",
    "- Intel 8 core CPU @ 3.6 GHZ\n",
    "- 32 GB Memory\n",
    "- Nvidia GEFORCE 2070 with 8 GB memory\n",
    "\n",
    "As you can see, larger models tend take longer to execute.\n",
    "\n",
    "You can also see GPU can really accelerate execution time!\n",
    "\n",
    "At the end, it is a trade off between **accuracy and performancee**\n",
    "\n",
    "| Model                                   | model size | embedding length | Execution time in ms (with GPU) | Execution time in ms (without GPU) |\n",
    "|-----------------------------------------|------------|------------------|---------------------------------|------------------------------------|\n",
    "| BAAI/bge-large-en-v1.5                  | 1.34 GB    | 1024             | 12.5                            | 67.7                               |\n",
    "| BAAI/bge-small-en-v1.5                  | 133 MB     | 384              | 6.5                             | 9.4                                |\n",
    "| sentence-transformers/all-mpnet-base-v2 | 438 MB     | 768              | 6.65                            | 20.5                               |\n",
    "| sentence-transformers/all-MiniLM-L12-v2 | 134 MB     | 384              | 6.74                            | 9.4                                |\n",
    "| sentence-transformers/all-MiniLM-L6-v2  | 91 MB      | 384              | 3.68                            | 4.8                                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model=BAAI/bge-large-en-v1.5, embeding_length=1,024\n",
      "12.6 ms ± 143 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "model=BAAI/bge-small-en-v1.5, embeding_length=384\n",
      "6.66 ms ± 179 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "model=sentence-transformers/all-mpnet-base-v2, embeding_length=768\n",
      "6.81 ms ± 107 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "model=sentence-transformers/all-MiniLM-L12-v2, embeding_length=384\n",
      "6.78 ms ± 149 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "model=sentence-transformers/all-MiniLM-L6-v2, embeding_length=384\n",
      "3.67 ms ± 157 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_models = [\n",
    "    'BAAI/bge-large-en-v1.5' ,\n",
    "    'BAAI/bge-small-en-v1.5' ,\n",
    "    'sentence-transformers/all-mpnet-base-v2' ,\n",
    "    'sentence-transformers/all-MiniLM-L12-v2' ,\n",
    "    'sentence-transformers/all-MiniLM-L6-v2' ,\n",
    "]\n",
    "\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "for model in embedding_models:\n",
    "    embed_model = HuggingFaceEmbedding(model_name=model)\n",
    "\n",
    "    embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
    "    print(f'model={model}, embeding_length={len(embeddings):,}')\n",
    "    %timeit (embed_model.get_text_embedding(\"Hello World!\"))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Encoding Using Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n",
      "Use pytorch device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 269.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      "  (2): Normalize()\n",
      ")\n",
      "384\n",
      "[-0.00542399  0.07206922 -0.02727443  0.04371347 -0.0695779 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "#sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# embeddings = model.encode(sentences)\n",
    "embeddings = model.encode('a happy dog!')\n",
    "print(model)\n",
    "print (len(embeddings))\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
